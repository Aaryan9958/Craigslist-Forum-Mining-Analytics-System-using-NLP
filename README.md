# Craigslist Forum Mining Analytics

**Goal:** Turn unstructured feedback from Craigslist forums into structured, actionable product insights using an endâ€‘toâ€‘end NLP pipeline (scraping â†’ preprocessing â†’ sentiment â†’ topics â†’ clustering â†’ supervised classification â†’ dashboard assets).

---

## ğŸ“¦ Repository Structure

```
craigslist-forum-analytics/
â”œâ”€ data/                # (optional) sample data or exports
â”œâ”€ models/              # trained models (.pkl) â€” large files are gitignored by default
â”œâ”€ notebooks/           # Jupyter notebooks (final_total_code.ipynb)
â”œâ”€ src/                 # optional: scripts if you modularize the pipeline
â”œâ”€ docs/                # Milestone reports & presentation PDFs
â”œâ”€ images/              # export charts/wordclouds here for README
â”œâ”€ requirements.txt     # Python dependencies
â”œâ”€ .gitignore
â”œâ”€ LICENSE
â””â”€ README.md
```

## ğŸ§  Project Overview

Craigslist hosts public forums (Feedback, Flag Help, Help Desk) with rich user comments. This project mines those discussions to surface **sentiment, recurring pain points, and themes** so product & moderation teams can prioritize fixes and improvements.

**Pipeline:**
1) **Web Scraping** â€” `requests`, `BeautifulSoup` to collect forum threads/posts  
2) **Preprocessing** â€” lowercase, deâ€‘noise, stopword removal, lemmatization  
3) **Sentiment Analysis** â€” VADER baseline + *frustration keyword override* for better complaint detection  
4) **Topic Modeling** â€” LDA (gensim) to discover themes (e.g., posting/visibility, UX/navigation, spam/messaging, etc.)  
5) **Vectorization & Clustering** â€” TFâ€‘IDF (1â€“2 grams) â†’ TruncatedSVD (100 comps) â†’ KMeans (kâ‰ˆ5) + silhouette  
6) **Supervised Models** â€” Logistic, SVM, RandomForest â†’ **Hard Voting Ensemble** for final classification  
7) **Reporting** â€” confusion matrices, topic/cluster keywords, word clouds, and a summarized dashboard

---

## ğŸš€ Quickstart

1) **Create a virtual environment** and install dependencies
```bash
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt
python -m nltk.downloader vader_lexicon stopwords wordnet punkt
```

2) **Open the notebook**
```bash
jupyter notebook notebooks/final_total_code.ipynb
```
Run cells topâ€‘toâ€‘bottom. The notebook covers scraping (or loading saved data), preprocessing, modeling, and evaluation.

---

## ğŸ“Š Results (at a glance)

- **Topics**: recurring issues around post visibility/flagging, UX/navigation, formatting, spam/messaging, and mixed sentiment about simplicity
- **Classifier**: Voting Ensemble (Logit + SVM + RF) achieved solid performance (accuracy and macroâ€‘F1 in the 70s range) with strong recall on negatives (helpful for complaint triage)
- **Artifacts**: confusion matrix, topic keyword tables, word clouds, explainedâ€‘variance plot.

---

## ğŸ–¼ï¸ Visuals
[View Title Slide](images/slide_01.jpg) Â·
[Pipeline](images/slide_02.jpg) Â·
[Scraping & Preprocessing](images/slide_03.jpg) Â·
[Sentiment](images/slide_04.jpg) Â·
[Topics](images/slide_05.jpg)




---

## ğŸ“¥ Data & Models

- Place sample/scraped data under `/data` (keep sensitive/raw data out of the repo).  
- Trained model files live in `/models`. Consider **Git LFS** if you need to version binary artifacts.

---

## ğŸ§ª Repro Notes

- If scraping, throttle requests and respect robots.txt/ToS.  
- For consistent results, fix random seeds where possible and note library versions.  
- Track parameters (e.g., number of topics, k for KMeans) in notebook cells or a config file.

---

## ğŸ›¡ï¸ License

MIT â€” see `LICENSE`.
